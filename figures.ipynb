{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to import hypothesis in common_utils, tests are not derandomized\n"
     ]
    }
   ],
   "source": [
    "from benchmarks import Experiment\n",
    "import logging\n",
    "import os\n",
    "from functools import wraps\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "import torch.fx as fx\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "from typing import Any\n",
    "\n",
    "from graph_prof import GraphProfiler\n",
    "from graph_tracer import SEPFunction, compile\n",
    "\n",
    "model_names: list[str] = [\n",
    "    \"Transformer\",\n",
    "    \"Resnet18\",\n",
    "    \"Resnet50\",\n",
    "]\n",
    "\n",
    "model_batch_sizes: dict[str, list[int]] = {\n",
    "    \"Transformer\": [4, 8, 16, 32],\n",
    "    \"Resnet18\": [4, 8, 16, 32],\n",
    "    \"Resnet50\": [4, 8, 16, 32],\n",
    "}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def plot_memory_usage(model_name, batch_size):\n",
    "    data_path = f\"results/{model_name}_batch{batch_size}_graph_profiler_stats.json\"\n",
    "    json_data = json.load(open(data_path))\n",
    "    df = pd.DataFrame(json_data)\n",
    "    \n",
    "    # Convert memory usage columns to GB\n",
    "    columns_to_plot = ['gpu_memory_usages', 'param_memory_usages', 'activation_memory_usages', 'grad_memory_usages', 'other_memory_usages']\n",
    "    df[columns_to_plot] = df[columns_to_plot] / 1e9  # Convert bytes to GB\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for column in columns_to_plot:\n",
    "        plt.plot(df[column], label=column)\n",
    "\n",
    "    plt.axvline(x=df.rank_of_backward[0], color='grey', linestyle='--', label='end of forward pass')\n",
    "\n",
    "    plt.xlabel('Steps', fontsize=14)\n",
    "    plt.ylabel('Memory Usage (GB)', fontsize=14)  # Updated to reflect GB\n",
    "    plt.title(f'Memory Usage for {model_name} with batch size {batch_size} over one training epoch', fontsize=18, weight='bold')\n",
    "\n",
    "    plt.legend(fontsize=12, loc='upper right', bbox_to_anchor=(1, 1))  \n",
    "    \n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)  # Reduce margins\n",
    "    \n",
    "    plt.savefig(f\"results/{model_name}_batch{batch_size}_graph_profiler_stats.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_memory_usage_versus_batch_size(model_name):\n",
    "    batch_sizes = [str(batch_size) for batch_size in model_batch_sizes[model_name]]\n",
    "    peak_memory_usages = []\n",
    "    for batch_size in batch_sizes:\n",
    "        data_path = f\"results/{model_name}_batch{batch_size}_graph_profiler_stats.json\"\n",
    "        json_data = json.load(open(data_path))\n",
    "        peak_memory_usages.append(json_data['peak_memory_usage'] / 1e9)  # Convert bytes to GB\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.bar(batch_sizes, peak_memory_usages)\n",
    "\n",
    "    plt.xticks(batch_sizes, fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    plt.xlabel('Batch Size', fontsize=14)\n",
    "    plt.ylabel('Peak Memory Usage (GB)', fontsize=14)  # Updated to reflect GB\n",
    "    plt.title(f'Peak Memory Usage for {model_name} over different batch sizes', fontsize=18, weight='bold')\n",
    "\n",
    "    plt.subplots_adjust(left=0.1, right=0.95, top=0.9, bottom=0.15)  # Reduce margins\n",
    "    \n",
    "    plt.savefig(f\"results/{model_name}_peak_memory_usage.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Experiment.__init__() got an unexpected keyword argument 'to_recompute'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m model_names:\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch_size \u001b[38;5;129;01min\u001b[39;00m model_batch_sizes[model_name]:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         exp = \u001b[43mExperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_recompute\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m         exp.init_opt_states()\n\u001b[32m      5\u001b[39m         compiled_fn = \u001b[38;5;28mcompile\u001b[39m(exp.train_step, exp.graph_transformation)\n",
      "\u001b[31mTypeError\u001b[39m: Experiment.__init__() got an unexpected keyword argument 'to_recompute'"
     ]
    }
   ],
   "source": [
    "for model_name in model_names:\n",
    "    for batch_size in model_batch_sizes[model_name]:\n",
    "        exp = Experiment(model_name, batch_size, to_recompute=False)\n",
    "        exp.init_opt_states()\n",
    "        compiled_fn = compile(exp.train_step, exp.graph_transformation)\n",
    "        compiled_fn(exp.model, exp.optimizer, exp.example_inputs)\n",
    "\n",
    "        plot_memory_usage(model_name, batch_size)\n",
    "    \n",
    "    plot_memory_usage_versus_batch_size(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs265",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
